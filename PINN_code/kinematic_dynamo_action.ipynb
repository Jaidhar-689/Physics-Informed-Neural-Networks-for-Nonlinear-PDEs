{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM/WSPAAdaPNi74QHPCK3Pz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## üß† Physics-Informed Neural Network for the 3D Magnetic Induction Equation\n","\n","This PINN implements the **kinematic dynamo model** studied by\n","**V. Archontis, S. B. F. Dorch, and √Ö. Nordlund (2003), _A&A 397, 393‚Äì399_**.\n","The governing equation is the **magnetic induction equation**:\n","\n","$$\n","\\frac{\\partial \\mathbf{B}}{\\partial t}\n","= \\nabla \\times (\\mathbf{u} \\times \\mathbf{B})\n","+ \\frac{1}{Re_m} \\nabla^2 \\mathbf{B},\n","\\qquad \\nabla \\cdot \\mathbf{B} = 0\n","$$\n","\n","where\n","- **$ \\mathbf{B}(x,y,z,t) = (B_x,B_y,B_z) $** is the magnetic field,\n","- **$ \\mathbf{u}(x,y,z) $** is a prescribed incompressible velocity field,\n","- **$ Re_m $** is the magnetic Reynolds number.\n","\n","---\n","\n","### ‚öôÔ∏è Velocity Field ‚Äì ABC Flow\n","The prescribed steady velocity is the **Arnold‚ÄìBeltrami‚ÄìChildress (ABC) flow**:\n","\n","$$\n","\\mathbf{u}_{ABC} =\n","\\big(A\\sin(kz) + C\\cos(ky),\\;\n","B\\sin(kx) + A\\cos(kz),\\;\n","C\\sin(ky) + B\\cos(kx)\\big),\n","$$\n","\n","with parameters $A = B = C = 1$ and $k = 1$.\n","This flow is incompressible and periodic on the domain $[0,2\\pi]^3$.\n","\n","---\n","\n","### üß© PINN Architecture\n","- **Input:** spatial‚Äìtemporal coordinates $(x, y, z, t)$\n","- **Output:** magnetic field components $(B_x, B_y, B_z)$\n","- **Network:** fully connected MLP with 4 hidden layers √ó 64 neurons\n","(activation: **Tanh**)\n","- **Loss function:**\n","  1. **PDE residual loss** enforcing\n","  $\\partial_t \\mathbf{B} - \\nabla\\times(\\mathbf{u}\\times\\mathbf{B}) - \\frac{1}{Re_m}\\nabla^2\\mathbf{B} = 0$\n","  2. **Divergence-free loss** enforcing $\\nabla\\cdot\\mathbf{B}=0$\n","  3. **Initial-condition loss** ensuring $\\mathbf{B}(x,y,z,0)=\\mathbf{B}_0(x,y,z)$\n","- **Optimizer:** Adam with StepLR scheduler (decay every 500 epochs)\n","- **Training points:**\n","  - Collocation points ($N_f$) sampled uniformly in the 4-D domain\n","  - Initial-condition points ($N_0$) at $t=0$\n","- **Domain:** $ x,y,z \\in [0,2\\pi] $, $t \\in [0,T_{\\text{max}}]$\n","\n","---\n","\n","### üìà Physical Meaning\n","The network learns a magnetic field evolution that simultaneously:\n","- satisfies the **induction equation** everywhere in the domain,\n","- preserves **solenoidality** ($\\nabla\\cdot\\mathbf{B}=0$),\n","- matches the **initial seed field** (small random perturbation).\n","\n","This mimics the **kinematic dynamo regime**, where magnetic fields grow\n","exponentially through stretching and folding by the ABC flow,\n","as described in the reference paper.\n","\n","---\n","\n","### üìä Visualization & Outputs\n","The training script plots:\n","- **Total loss** vs. epoch\n","- **Magnetic energy** ($E_m = \\tfrac{1}{2}\\langle|\\mathbf{B}|^2\\rangle$) vs. epoch\n","- **|B| field slice** at $ z=\\pi $, $t=T_{\\text{max}}$\n","\n","---\n","\n","### üßæ Reference\n","Archontis V., Dorch S. B. F., Nordlund √Ö. (2003).\n","**\"Numerical simulations of kinematic dynamo action.\"**\n","\n","_Astronomy & Astrophysics_ [https://www.aanda.org/articles/aa/pdf/2003/02/aa2653.pdf](https://www.aanda.org/articles/aa/pdf/2003/02/aa2653.pdf)"],"metadata":{"id":"NP8mxHcNiVj4"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KH7Tfy29ahRO","executionInfo":{"status":"ok","timestamp":1762415293973,"user_tz":-330,"elapsed":32167,"user":{"displayName":"Jaidhar Ramagiri","userId":"09689778840436794923"}},"outputId":"fe9d6fa8-d1e7-423a-c799-49b6a27b67a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import linregress\n"],"metadata":{"id":"SHJ86j3Ta6CG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =======================================\n","# Device setup\n","# =======================================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0Lykbpma61Z","executionInfo":{"status":"ok","timestamp":1762415299711,"user_tz":-330,"elapsed":43,"user":{"displayName":"Jaidhar Ramagiri","userId":"09689778840436794923"}},"outputId":"eb6fce9f-6353-4441-da59-10fa65a29dd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"code","source":["# Neural Network for Magnetic Induction Equation (Vector B)\n","\n","class PINN_Magnetic3D(nn.Module):\n","    def __init__(self, layers, dropout_prob=0.1):\n","        super(PINN_Magnetic3D, self).__init__()\n","        self.layers = nn.ModuleList()\n","        # self.dropout = nn.Dropout(p=dropout_prob)\n","        for i in range(len(layers) - 1):\n","            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n","        self.activation = nn.Tanh()\n","\n","    def forward(self, x, y, z, t):\n","        inp = torch.cat([x, y, z, t], dim=1)\n","        for i in range(len(self.layers) - 2):  # all hidden layers except last\n","            inp = self.activation(self.layers[i](inp))\n","            inp = self.dropout(inp)  # apply dropout after activation\n","        out = self.layers[-1](inp)\n","        return out  # (N, 3)\n","\n","\n","def to_tensor(x):\n","    return torch.as_tensor(x, dtype=torch.float32, device=device)\n"],"metadata":{"id":"Ewdp8lJ4a-H8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# Differential helpers\n","# -------------------------\n","def gradient(f, x):\n","    # returns df/dx with same shape as f\n","    return torch.autograd.grad(f, x, torch.ones_like(f), retain_graph=True, create_graph=True)[0]\n","\n","def laplacian_scalar(f, x, y, z):\n","    # compute d^2 f / dx^2 + d^2 f / dy^2 + d^2 f / dz^2\n","    f_x = gradient(f, x)\n","    f_xx = gradient(f_x, x)\n","    f_y = gradient(f, y)\n","    f_yy = gradient(f_y, y)\n","    f_z = gradient(f, z)\n","    f_zz = gradient(f_z, z)\n","    return f_xx + f_yy + f_zz\n","\n","def curl_from_components(vx, vy, vz, x, y, z):\n","    # curl = (dvz/dy - dvy/dz, dvx/dz - dvz/dx, dvy/dx - dvx/dy)\n","    vz_y = gradient(vz, y)\n","    vy_z = gradient(vy, z)\n","    vx_z = gradient(vx, z)\n","    vz_x = gradient(vz, x)\n","    vy_x = gradient(vy, x)\n","    vx_y = gradient(vx, y)\n","    curl_x = vz_y - vy_z\n","    curl_y = vx_z - vz_x\n","    curl_z = vy_x - vx_y\n","    return curl_x, curl_y, curl_z\n","\n","# -------------------------\n","# PDE residual for induction eq\n","# -------------------------\n","def pde_residual(model, u_func, x, y, z, t, Rem, A, B, C, k_abc):\n","    # ensure grads\n","    x.requires_grad_(True); y.requires_grad_(True); z.requires_grad_(True); t.requires_grad_(True)\n","\n","    B = model(x, y, z, t)         # (N,3)\n","    Bx = B[:, 0:1]; By = B[:, 1:2]; Bz = B[:, 2:3]\n","\n","    # time derivatives\n","    Bx_t = gradient(Bx, t)\n","    By_t = gradient(By, t)\n","    Bz_t = gradient(Bz, t)\n","\n","    # velocity\n","    ux, uy, uz = u_func(x, y, z, A, B, C, k_abc)\n","\n","    # u √ó B\n","    uxB_x = uy * Bz - uz * By\n","    uxB_y = uz * Bx - ux * Bz\n","    uxB_z = ux * By - uy * Bx\n","\n","    # curl(u √ó B)\n","    curl_x, curl_y, curl_z = curl_from_components(uxB_x, uxB_y, uxB_z, x, y, z)\n","\n","    # laplacian(B)\n","    lapBx = laplacian_scalar(Bx, x, y, z)\n","    lapBy = laplacian_scalar(By, x, y, z)\n","    lapBz = laplacian_scalar(Bz, x, y, z)\n","\n","    # residuals\n","    fBx = Bx_t - curl_x - (1.0/Rem) * lapBx\n","    fBy = By_t - curl_y - (1.0/Rem) * lapBy\n","    fBz = Bz_t - curl_z - (1.0/Rem) * lapBz\n","\n","    # divergence free\n","    divBx = gradient(Bx, x)\n","    divBy = gradient(By, y)\n","    divBz = gradient(Bz, z)\n","    divB = divBx + divBy + divBz\n","\n","    return fBx, fBy, fBz, divB\n","\n"],"metadata":{"id":"ZkuMuO-AhEQE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def abc_flow(x, y, z, A, B, C, k):\n","    ux = A * (0*x + 0) + B * torch.cos(k*y) + C * torch.sin(k*z)\n","    uy = A * torch.sin(k*x) + B * (0*y + 0) + C * torch.cos(k*z)\n","    uz = A * torch.cos(k*x) + B * torch.sin(k*y) + C * (0*z + 0)\n","    return ux, uy, uz\n"],"metadata":{"id":"MXKD6A6ua1rp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# Problem hyperparams\n","# -------------------------\n","\n","Re_m = 100.0               # magnetic Reynolds number (can change)\n","A, B, C = 1.0, 1.0, 1.0     # ABC flow amplitudes\n","k_abc = 2                   # wavenumber\n","domain_min = 0.0\n","domain_max = 2*np.pi        # use 0..2œÄ periodic domain\n","T_max = 40.0                 # final time (choose small for testing)\n","\n","N_f = 2500\n","N0  = 500\n","lr = 1e-3\n","\n","# -------------------------\n","# Initial condition: small random seed (like paper)\n","# -------------------------\n","def initial_B_random(x, y, z, amp=1e-5):\n","    # small random vector seed at t=0\n","    # x,y,z are numpy arrays (N,1) when generating dataset\n","    N = x.shape[0]\n","    B0 = amp * (np.random.randn(N,3))\n","    return B0\n","\n","# -------------------------\n","# Dataset (random collocation inside domain)\n","# -------------------------\n","def sample_domain(N, t_min=0.0, t_max=T_max):\n","    x = np.random.uniform(domain_min, domain_max, (N,1))\n","    y = np.random.uniform(domain_min, domain_max, (N,1))\n","    z = np.random.uniform(domain_min, domain_max, (N,1))\n","    t = np.random.uniform(t_min, t_max, (N,1))\n","    return x, y, z, t\n","\n","\n","# initial IC points\n","x0_np, y0_np, z0_np = (np.random.uniform(domain_min, domain_max, (N0,1)) for _ in range(3))\n","t0_np = np.zeros((N0,1))\n","B0_np = initial_B_random(x0_np, y0_np, z0_np, amp=1e-5)\n","\n","# convert to tensors\n","x0_t = to_tensor(x0_np); y0_t = to_tensor(y0_np); z0_t = to_tensor(z0_np); t0_t = to_tensor(t0_np)\n","B0_t = to_tensor(B0_np)\n","\n","# -------------------------\n","# Model, optimizer\n","# -------------------------\n","model = PINN_Magnetic3D([4, 64, 64, 64, 3]).to(device)\n","# model = PINN_Magnetic3D([4, 64, 64, 64, 3], dropout_prob=0.05).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n","\n","# -------------------------\n","# Training loop\n","# -------------------------\n","loss_history = []\n","mag_energy_history = []\n","epochs = 10000  # training epochs\n","\n","\n","for ep in range(epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    # sample collocation points each epoch (physics-informed collocation)\n","    x_f_np, y_f_np, z_f_np, t_f_np = sample_domain(N_f)\n","    x_f = to_tensor(x_f_np, requires_grad=True)\n","    y_f = to_tensor(y_f_np, requires_grad=True)\n","    z_f = to_tensor(z_f_np, requires_grad=True)\n","    t_f = to_tensor(t_f_np, requires_grad=True)\n","\n","    fBx, fBy, fBz, divB = pde_residual(model, abc_flow, x_f, y_f, z_f, t_f, Re_m, A, B, C, k_abc)\n","    loss_pde = torch.mean(fBx**2 + fBy**2 + fBz**2)\n","    # loss_pde = torch.mean(torch.sqrt(fBx**2 + fBy**2 + fBz**2))\n","\n","    loss_div = torch.mean(divB**2)\n","\n","    B0_pred = model(x0_t, y0_t, z0_t, t0_t)\n","    loss_ic = torch.mean((B0_pred - B0_t)**2)\n","\n","    loss = 10.0*loss_pde + 10.0*loss_div + 1.0*loss_ic  # weights can be tuned\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","\n","    loss_history.append(loss.item())\n","\n","    if ep % 250 == 0:\n","        print(f\"[{ep:5d}] loss: {loss.item():.3e}  loss_pde:{loss_pde.item():.3e} loss_div:{loss_div.item():.3e} loss_ic:{loss_ic.item():.3e} lr: {scheduler.get_last_lr()[0]:.3e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BFPYg3yAh9Fy","executionInfo":{"status":"error","timestamp":1762415747847,"user_tz":-330,"elapsed":448069,"user":{"displayName":"Jaidhar Ramagiri","userId":"09689778840436794923"}},"outputId":"22bd9c05-4e1c-4697-994c-4f2222103ccc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[    0] loss: 1.381e+02  loss_pde:1.381e+01 loss_div:1.475e-04 loss_ic:6.579e-03 lr: 1.000e-03\n","[  250] loss: 2.189e-02  loss_pde:2.118e-03 loss_div:1.931e-05 loss_ic:5.224e-04 lr: 1.000e-03\n","[  500] loss: 8.917e-03  loss_pde:8.622e-04 loss_div:9.829e-06 loss_ic:1.965e-04 lr: 5.000e-04\n","[  750] loss: 6.015e-03  loss_pde:5.819e-04 loss_div:6.257e-06 loss_ic:1.335e-04 lr: 5.000e-04\n","[ 1000] loss: 4.292e-03  loss_pde:4.147e-04 loss_div:5.036e-06 loss_ic:9.457e-05 lr: 2.500e-04\n","[ 1250] loss: 3.739e-03  loss_pde:3.613e-04 loss_div:4.454e-06 loss_ic:8.199e-05 lr: 2.500e-04\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3925603261.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_pde\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_div\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_ic\u001b[0m  \u001b[0;31m# weights can be tuned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/NN_Models\"\n","m_name = \"/kinematic_dynamo_action_10k_tanh_NN_10lossicW_Rem100\"\n","torch.save(model, path + m_name + \".pt\")"],"metadata":{"id":"bOhwZviabB2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# Plot loss\n","# -----------------------------\n","plt.figure(figsize=(8,5))\n","plt.plot(loss_history, label=\"Training Loss (Adam)\")\n","plt.yscale(\"log\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss vs Epochs \")\n","plt.legend()\n","plt.grid(True)\n","plt.savefig(path + m_name + \"_Loss.png\")  # Save the plot as a PNG file\n","plt.show()"],"metadata":{"id":"nFFEWAiQUuth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# Visualize B magnitude slice at final time T_max, z=pi\n","# -------------------------\n","with torch.no_grad():\n","    Nx = 120\n","    xs = np.linspace(domain_min, domain_max, Nx)\n","    ys = np.linspace(domain_min, domain_max, Nx)\n","    X, Y = np.meshgrid(xs, ys)\n","    Z = np.full_like(X, domain_max/2.0)\n","    Tt = np.full_like(X, T_max)\n","    xt = to_tensor(X.flatten()[:,None]); yt = to_tensor(Y.flatten()[:,None])\n","    zt = to_tensor(Z.flatten()[:,None]); tt = to_tensor(Tt.flatten()[:,None])\n","    Bf = model(xt, yt, zt, tt).cpu().numpy()\n","    Bmag = np.sqrt(np.sum(Bf**2, axis=1)).reshape(Nx,Nx)\n","\n","plt.figure(figsize=(6,5))\n","plt.contourf(X, Y, Bmag, levels=60)\n","plt.colorbar(label='|B|')\n","plt.title(f'|B|(z={domain_max/2.0:.2f}, t={T_max})')\n","plt.xlabel('x'); plt.ylabel('y')\n","plt.show()"],"metadata":{"id":"2xvZkxSCTxlU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# Plot Emag vs Time (Post-training analysis)\n","# -------------------------\n","print(\"\\nCalculating magnetic energy evolution over time...\")\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Define the grid for calculating the volume-averaged energy\n","N_grid = 32 # Spatial resolution for the grid (32x32x32). Lower for faster computation.\n","x_dom = np.linspace(domain_min, domain_max, N_grid)\n","y_dom = np.linspace(domain_min, domain_max, N_grid)\n","z_dom = np.linspace(domain_min, domain_max, N_grid)\n","X, Y, Z = np.meshgrid(x_dom, y_dom, z_dom)\n","\n","# Flatten and convert the spatial grid to tensors once\n","x_vol = to_tensor(X.flatten()[:, None])\n","y_vol = to_tensor(Y.flatten()[:, None])\n","z_vol = to_tensor(Z.flatten()[:, None])\n","\n","# Define time steps for the plot\n","t_steps = 1000\n","t_plot = np.linspace(0, T_max + 10, t_steps)\n","emag_vs_time = []\n","\n","with torch.no_grad():\n","    for t_val in t_plot:\n","        # Create a time tensor for the current time step\n","        t_vol = to_tensor(np.full_like(X.flatten()[:, None], t_val))\n","\n","        # Predict the magnetic field B at this time step over the entire volume\n","        B_pred = model(x_vol, y_vol, z_vol, t_vol).cpu().numpy()\n","\n","        # Calculate the mean magnetic energy for this time step\n","        # Emag = 0.5 * <B^2> integrated over volume\n","        emag_current = 0.5 * np.mean(np.sum(B_pred**2, axis=1))\n","        emag_vs_time.append(emag_current)\n","\n","        # Optional: print progress\n","        if len(emag_vs_time) % 10 == 0:\n","            # print(f\"  Computed for t = {t_val:.2f}, Emag = {emag_current:.4e}\")\n","            pass\n","\n","# Create the plot\n","plt.figure(figsize=(8, 6))\n","# emag_vs_time = np.log(emag_vs_time)\n","plt.plot(t_plot, emag_vs_time)#, marker='o', markersize=4, linestyle='-')\n","plt.yscale('log')  # Dynamo action is exponential\n","plt.xlabel(\"Time ($t$)\")\n","plt.ylabel(\"Magnetic Energy ($E_{mag}$)\")\n","plt.title(\"Magnetic Energy Evolution Over Time (Trained Model)\")\n","plt.grid(True)\n","plt.tight_layout()\n","plt.savefig(path + m_name + \"_EvsT_Plot.png\")  # Save the plot as a PNG file\n","plt.show()"],"metadata":{"id":"XaflJpeDogC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1A: Check B magnitude distribution at t=0 and later\n","model.eval()\n","with torch.no_grad():\n","    # pick a small set of random points\n","    idx = np.random.choice(x_vol.shape[0], size=1000, replace=False)\n","    pts_x = x_vol[idx]; pts_y = y_vol[idx]; pts_z = z_vol[idx]\n","    t0 = to_tensor(np.zeros((len(idx),1)))\n","    tlate = to_tensor(np.full((len(idx),1), T_max))\n","\n","    B0 = model(pts_x, pts_y, pts_z, t0).cpu().numpy()   # shape (N,3)\n","    Blate = model(pts_x, pts_y, pts_z, tlate).cpu().numpy()\n","\n","print(\"B0 min/max, mean |B|:\", np.min(np.linalg.norm(B0,axis=1)),\n","      np.max(np.linalg.norm(B0,axis=1)), np.mean(np.linalg.norm(B0,axis=1)))\n","print(\"B(T) min/max, mean |B|:\", np.min(np.linalg.norm(Blate,axis=1)),\n","      np.max(np.linalg.norm(Blate,axis=1)), np.mean(np.linalg.norm(Blate,axis=1)))\n"],"metadata":{"id":"tbxES5YPbCP9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sample u values\n","x_s = to_tensor(np.array([[0.1],[1.2],[2.5]]))\n","y_s = x_s; z_s = x_s\n","print(\"u sample:\", abc_flow(x_s,y_s,z_s, A, B, C, k_abc)[:5])\n"],"metadata":{"id":"XD_22NYGbI4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# single point probe\n","x, y, z = 0,0,0\n","# x, y, z = np.pi, np.pi, np.pi\n","# x, y, z = np.pi/4, np.pi/3, np.pi/6\n","# x, y, z = 2*np.pi, 2*np.pi, 2*np.pi\n","xp = to_tensor(np.array([[x]]))\n","yp = to_tensor(np.array([[y]]))\n","zp = to_tensor(np.array([[z]]))\n","bvals = []\n","for tval in t_plot:\n","    tb = to_tensor(np.array([[tval]]))\n","    with torch.no_grad():\n","        b = model(xp,yp,zp,tb).cpu().numpy().ravel()\n","    bvals.append(np.linalg.norm(b))\n","bvals = np.array(bvals)\n","print(\"bvals min/max:\", bvals.min(), bvals.max(), \"ratio max/min:\", bvals.max()/max(bvals.min(),1e-30))\n","plt.semilogy(t_plot, bvals); plt.title(\"B norm at single probe\"); plt.show()\n"],"metadata":{"id":"Pf9Y4JbPhTSL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logE = np.log(emag_vs_time)\n","growth_rate = np.gradient(logE, t_plot)\n","print(\"Average growth rate:\", np.mean(growth_rate[t_plot>5]))\n"],"metadata":{"id":"R23LCgg4o0j5"},"execution_count":null,"outputs":[]}]}